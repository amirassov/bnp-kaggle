{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import Imputer, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.manifold as sm\n",
    "import scipy.sparse as sps\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feature_preprocessing as fproc\n",
    "import feature_extraction as fext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "Y_train = df_train['target'].values\n",
    "id_test = df_test['ID'].values\n",
    "id_train = df_train['ID'].values\n",
    "\n",
    "df_train = df_train.drop(['ID','target'],axis=1)\n",
    "df_test = df_test.drop(['ID'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_features = set(['v8','v23','v25','v31','v36','v37','v46',\n",
    "                'v51','v53','v54','v63','v73','v75','v79','v81','v82',\n",
    "                'v89','v92','v95','v105','v107','v108','v109','v110',\n",
    "                'v116','v117','v118','v119','v123','v124','v128'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_features = ['v3', 'v22', 'v24', 'v30', 'v31', 'v47', 'v52', 'v56', 'v66', 'v71', \n",
    "                'v74', 'v75', 'v79', 'v91', 'v107', 'v110', 'v112', 'v113', 'v125']\n",
    "num_features = ['v18', 'v19', 'v12', 'v13', 'v10', 'v11','v16', 'v17', \n",
    "                'v14', 'v15', 'v118', 'v119', 'v114', 'v115', 'v116', \n",
    "                'v117', 'v111','v89', 'v88', 'v85', 'v84', 'v87', 'v86', \n",
    "                'v81', 'v80', 'v83', 'v82', 'v69', 'v68', 'v67', 'v65', \n",
    "                'v64', 'v63', 'v62', 'v61', 'v60', 'v92', 'v93', 'v90', \n",
    "                'v96', 'v97', 'v94', 'v95', 'v106', 'v98', 'v104', 'v103', \n",
    "                'v102', 'v101', 'v100', 'v105', 'v99', 'v78', 'v76', 'v77', \n",
    "                'v70', 'v72', 'v73', 'v130', 'v131', 'v124', 'v127', 'v126', \n",
    "                'v121', 'v120', 'v123', 'v122', 'v129', 'v128', 'v41', 'v40', \n",
    "                'v43', 'v42', 'v45', 'v44', 'v46', 'v49', 'v48', 'v23', 'v21',\n",
    "                'v20', 'v27', 'v26', 'v25', 'v29', 'v28', 'v57', 'v54', 'v55', \n",
    "                'v53', 'v50', 'v51', 'v109', 'v58', 'v59', 'v32', 'v33', 'v34', \n",
    "                'v35', 'v36', 'v37', 'v38', 'v39', 'v108', 'v1', 'v2', \n",
    "                'v4', 'v5', 'v6', 'v7', 'v8', 'v9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dcat_features = list(set(cat_features) - drop_features)\n",
    "dnum_features = list(set(num_features) - drop_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(list(drop_features),axis=1)\n",
    "df_test = df_test.drop(list(drop_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v30</th>\n",
       "      <th>v22</th>\n",
       "      <th>v91</th>\n",
       "      <th>v24</th>\n",
       "      <th>v74</th>\n",
       "      <th>v66</th>\n",
       "      <th>v71</th>\n",
       "      <th>v112</th>\n",
       "      <th>v113</th>\n",
       "      <th>v125</th>\n",
       "      <th>v56</th>\n",
       "      <th>v3</th>\n",
       "      <th>v52</th>\n",
       "      <th>v47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>54211</td>\n",
       "      <td>113821</td>\n",
       "      <td>114318</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>114321</td>\n",
       "      <td>113939</td>\n",
       "      <td>59017</td>\n",
       "      <td>114244</td>\n",
       "      <td>107439</td>\n",
       "      <td>110864</td>\n",
       "      <td>114318</td>\n",
       "      <td>114321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>18210</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>36</td>\n",
       "      <td>90</td>\n",
       "      <td>122</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>C</td>\n",
       "      <td>AGDF</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>G</td>\n",
       "      <td>BM</td>\n",
       "      <td>BW</td>\n",
       "      <td>C</td>\n",
       "      <td>J</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>32178</td>\n",
       "      <td>2386</td>\n",
       "      <td>27079</td>\n",
       "      <td>55177</td>\n",
       "      <td>113560</td>\n",
       "      <td>70353</td>\n",
       "      <td>75094</td>\n",
       "      <td>21671</td>\n",
       "      <td>16252</td>\n",
       "      <td>5759</td>\n",
       "      <td>11351</td>\n",
       "      <td>110584</td>\n",
       "      <td>11103</td>\n",
       "      <td>55425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          v30     v22     v91     v24     v74     v66     v71    v112   v113  \\\n",
       "count   54211  113821  114318  114321  114321  114321  114321  113939  59017   \n",
       "unique      7   18210       7       5       3       3       9      22     36   \n",
       "top         C    AGDF       A       E       B       A       F       F      G   \n",
       "freq    32178    2386   27079   55177  113560   70353   75094   21671  16252   \n",
       "\n",
       "          v125     v56      v3     v52     v47  \n",
       "count   114244  107439  110864  114318  114321  \n",
       "unique      90     122       3      12      10  \n",
       "top         BM      BW       C       J       C  \n",
       "freq      5759   11351  110584   11103   55425  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[cat_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C    55425\n",
       "I    39071\n",
       "E     5301\n",
       "F     4322\n",
       "G     3946\n",
       "D     3157\n",
       "J     3010\n",
       "B       50\n",
       "A       38\n",
       "H        1\n",
       "Name: v47, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['v47'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count\n",
    "v3, v74\n",
    "v91, v30\n",
    "\n",
    "v71: L + D + K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_denominator(df, col):\n",
    "    \"\"\"\n",
    "    Function that trying to find an approximate denominator used for scaling.\n",
    "    So we can undo the feature scaling.\n",
    "    \"\"\"\n",
    "    vals = df[col].dropna().sort_values().round(8)\n",
    "    vals = pd.rolling_apply(vals, 2, lambda x: x[1] - x[0])\n",
    "    vals = vals[vals > 1e-5]\n",
    "    \n",
    "    return vals.value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_scale(df_train, df_test, num_features):\n",
    "    df = pd.concat([df_train, df_test], axis=0)\n",
    "    for f in tqdm(num_features):\n",
    "        if f not in df_train.columns:\n",
    "            continue\n",
    "\n",
    "        df_train.loc[df_train[f].round(5) == 0, f] = 0\n",
    "        df_test.loc[df_test[f].round(5) == 0, f] = 0\n",
    "\n",
    "        denominator = find_denominator(df, f)\n",
    "        df_train[f] *= 1/denominator\n",
    "        df_test[f] *= 1/denominator\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86/86 [00:25<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = df_scale(df_train, df_test, dnum_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Количество пропущенных значений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. Количество пропущенных элементов в таблице с обучающей выборкой:  3813061.0\n",
      "2. Количество объектов имеющие хотя бы один пропуск:  96565\n",
      "3. Количество признаков имеющие хотя бы один пропуск:  91\n"
     ]
    }
   ],
   "source": [
    "is_missing = df_train.isnull().values.astype(int)\n",
    "print '1. Количество пропущенных элементов в таблице с обучающей выборкой: ', float(np.sum(is_missing))\n",
    "print '2. Количество объектов имеющие хотя бы один пропуск: ', np.sum(np.sum(is_missing, axis=1) > 0)\n",
    "print '3. Количество признаков имеющие хотя бы один пропуск: ', np.sum(np.sum(is_missing, axis=0) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Факторизация категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 65.20it/s]\n"
     ]
    }
   ],
   "source": [
    "factor = fproc.Factorize(nan_strategy=np.nan)\n",
    "\n",
    "fcat_features = list(set(cat_features).intersection(drop_features))\n",
    "fac_train = factor.fit_transform(df_train[fcat_features])\n",
    "fac_test = factor.transform(df_test[fcat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_cat = pd.concat((df_train[dcat_features], df_test[dcat_features]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:00<00:00, 24.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RankCount()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = fext.RankCount()\n",
    "ranker.fit(df_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 19.66it/s]\n",
      "14it [00:00, 19.91it/s]\n"
     ]
    }
   ],
   "source": [
    "rank_train = ranker.transform(df_train[dcat_features])\n",
    "rank_test = ranker.transform(df_test[dcat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NaN-features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nan_encoder = fext.NanEncoding()\n",
    "nan_train = nan_encoder.fit_transform(df_train)\n",
    "nan_test = nan_encoder.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Избавляемся от NaN'ов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp = fproc.SuperImputer(-999.0, 0, num_features, dcat_features)\n",
    "nonan_train = imp.fit_transform(pd.concat((df_train[num_features], rank_train + 1), axis=1))\n",
    "nonan_test = imp.transform(pd.concat((df_test[num_features], rank_test + 1), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rank_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-ce4a499ae824>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSuperImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m999.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m999.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrnonan_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mrnonan_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rank_train' is not defined"
     ]
    }
   ],
   "source": [
    "imp = fproc.SuperImputer(-999.0, 999.0, num_features, cat_features)\n",
    "rnonan_train = imp.fit_transform(pd.concat((df_train[num_features], rank_train), axis=1))\n",
    "rnonan_test = imp.transform(pd.concat((df_test[num_features], rank_test), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "norm_features = list(set(dcat_features) - set(['v22']))\n",
    "df_enc = encoder.fit_transform(pd.concat([nonan_train[norm_features], nonan_test[norm_features]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_train = df_enc.toarray()[:len(df_train)]\n",
    "enc_test = df_enc.toarray()[len(df_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = fproc.SuperImputer(-999.0, -999.0, num_features, dcat_features)\n",
    "nonan_train = imp.fit_transform(pd.concat((df_train[num_features], rank_train), axis=1))\n",
    "nonan_test = imp.transform(pd.concat((df_test[num_features], rank_test), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Счетчики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:09,  1.47it/s]\n",
      "14it [00:00, 45.21it/s]\n"
     ]
    }
   ],
   "source": [
    "counter = fext.CountEncoding(n_folds=7, verbose=True)\n",
    "count_train = counter.fit_transform(nonan_train[dcat_features], Y_train)\n",
    "count_test = counter.transform(nonan_test[dcat_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### СуперФичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp_features = ['v50', 'v10', 'v12', 'v14', 'v40', 'v114']\n",
    "norm_features = list(set(cat_features) - set(['v22']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imp = fproc.SuperImputer('mean', -999, imp_features, norm_features)\n",
    "no_train = imp.fit_transform(pd.concat((df_train[imp_features], fac_train[norm_features]), axis=1))\n",
    "no_test = imp.transform(pd.concat((df_test[imp_features], fac_test[norm_features]), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'feature_extraction' from 'feature_extraction.py'>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(fext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [10:59<00:00, 48.99s/it]\n"
     ]
    }
   ],
   "source": [
    "super_encoder = fext.SuperCatEncoder(cat_features=norm_features, num_features=imp_features)\n",
    "df_super = super_encoder.get(pd.concat([no_train, no_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "super_train = df_super[:len(df_train)]\n",
    "super_test = df_super[len(df_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Парные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [00:24,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "pair_train, pair_test = fext.make_pair_features(df_train[cat_features], df_test[cat_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [00:04<00:00, 45.28it/s]\n"
     ]
    }
   ],
   "source": [
    "factor = fproc.Factorize(nan_strategy=-999.0)\n",
    "fac_pair_train = factor.fit_transform(pair_train)\n",
    "fac_pair_test = factor.transform(pair_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "super_train.to_csv('super_train.csv', index=False)\n",
    "super_test.to_csv('super_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonan_train.to_csv('nonan_train.csv', index=False)\n",
    "nonan_test.to_csv('nonan_test.csv', index=False)\n",
    "\n",
    "fac_train.to_csv('fac_train.csv', index=False)\n",
    "fac_test.to_csv('fac_test.csv', index=False)\n",
    "\n",
    "fac_pair_train.to_csv('fac_pair_train.csv', index=False)\n",
    "fac_pair_test.to_csv('fac_pair_test.csv', index=False)\n",
    "\n",
    "nan_train.to_csv('nan_train.csv', index=False)\n",
    "nan_test.to_csv('nan_test.csv', index=False)\n",
    "\n",
    "count_train.to_csv('count_train.csv', index=False)\n",
    "count_test.to_csv('count_test.csv', index=False)\n",
    "\n",
    "pd.DataFrame(enc_train).to_csv('enc_train.csv', index=False)\n",
    "pd.DataFrame(enc_test).to_csv('enc_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downland data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "super_train = pd.read_csv('super_train.csv')\n",
    "super_test = pd.read_csv('super_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nonan_train = pd.read_csv('nonan_train.csv')\n",
    "nonan_test = pd.read_csv('nonan_test.csv')\n",
    "\n",
    "fac_train = pd.read_csv('fac_train.csv')\n",
    "fac_test = pd.read_csv('fac_test.csv')\n",
    "\n",
    "fac_pair_train = pd.read_csv('fac_pair_train.csv')\n",
    "fac_pair_test = pd.read_csv('fac_pair_test.csv')\n",
    "\n",
    "nan_train = pd.read_csv('nan_train.csv')\n",
    "nan_test = pd.read_csv('nan_test.csv')\n",
    "\n",
    "count_train = pd.read_csv('count_train.csv')\n",
    "count_test = pd.read_csv('count_test.csv')\n",
    "\n",
    "enc_train = pd.read_csv('enc_train.csv')\n",
    "enc_test = pd.read_csv('enc_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nocv_loss(X_train, Y_train, clf, n_run=5, reg=False):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3, random_state=44)\n",
    "    \n",
    "    clf.fit(x_train, y_train)\n",
    "    if reg:\n",
    "        pred = clf.predict(x_test)\n",
    "    else:\n",
    "        pred = clf.predict_proba(x_test)[:, 1]\n",
    "    print log_loss(y_test, pred)\n",
    "    return pred, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = sps.hstack((nonan_train, count_train, fac_pair_train, nan_train), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.458534101931\n",
      "Time: 19.3043187499\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "extc = ExtraTreesClassifier(n_estimators=1200,\n",
    "                            max_features=40,\n",
    "                            criterion='entropy',\n",
    "                            min_samples_split=2,\n",
    "                            max_depth=30, \n",
    "                            min_samples_leaf=2, \n",
    "                            n_jobs=-1)    \n",
    "\n",
    "clf_ext_preds, y_test = nocv_loss(nonan_train, Y_train, extc, 1)\n",
    "print 'Time:', (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v50'"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonan_train.columns[70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v50', 'v66', 'v113', 'v52', 'v10', 'v12', 'v91', 'v14', 'v40', 'v114']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp_features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature v50 (0.098883)\n",
      "feature v66 (0.029814)\n",
      "feature v113 (0.029281)\n",
      "feature v52 (0.026626)\n",
      "feature v10 (0.026609)\n",
      "feature v12 (0.025828)\n",
      "feature v91 (0.025454)\n",
      "feature v14 (0.024851)\n",
      "feature v40 (0.024734)\n",
      "feature v114 (0.024679)\n",
      "feature v125 (0.023512)\n",
      "feature v22 (0.023218)\n",
      "feature v34 (0.022912)\n",
      "feature v21 (0.021780)\n",
      "feature v24 (0.021645)\n",
      "feature v56 (0.021406)\n",
      "feature v112 (0.021309)\n",
      "feature v47 (0.018832)\n",
      "feature v71 (0.018186)\n",
      "feature v30 (0.016790)\n",
      "feature v62 (0.015922)\n",
      "feature v72 (0.012248)\n",
      "feature v129 (0.011290)\n",
      "feature v87 (0.007303)\n",
      "feature v98 (0.007297)\n",
      "feature v5 (0.006973)\n",
      "feature v70 (0.006927)\n",
      "feature v120 (0.006916)\n",
      "feature v58 (0.006846)\n",
      "feature v1 (0.006808)\n",
      "feature v131 (0.006657)\n",
      "feature v28 (0.006591)\n",
      "feature v100 (0.006564)\n",
      "feature v88 (0.006525)\n",
      "feature v99 (0.006333)\n",
      "feature v16 (0.006275)\n",
      "feature v85 (0.006239)\n",
      "feature v6 (0.006219)\n",
      "feature v127 (0.006117)\n",
      "feature v2 (0.006078)\n",
      "feature v102 (0.006056)\n",
      "feature v80 (0.006052)\n",
      "feature v69 (0.005969)\n",
      "feature v115 (0.005887)\n",
      "feature v39 (0.005868)\n",
      "feature v45 (0.005780)\n",
      "feature v57 (0.005744)\n",
      "feature v18 (0.005741)\n",
      "feature v122 (0.005732)\n",
      "feature v97 (0.005721)\n",
      "feature v84 (0.005690)\n",
      "feature v78 (0.005648)\n",
      "feature v9 (0.005639)\n",
      "feature v27 (0.005540)\n",
      "feature v43 (0.005531)\n",
      "feature v15 (0.005510)\n",
      "feature v60 (0.005484)\n",
      "feature v86 (0.005482)\n",
      "feature v7 (0.005456)\n",
      "feature v26 (0.005453)\n",
      "feature v55 (0.005452)\n",
      "feature v111 (0.005445)\n",
      "feature v90 (0.005426)\n",
      "feature v103 (0.005419)\n",
      "feature v101 (0.005396)\n",
      "feature v126 (0.005366)\n",
      "feature v32 (0.005320)\n",
      "feature v44 (0.005293)\n",
      "feature v35 (0.005277)\n",
      "feature v121 (0.005203)\n",
      "feature v4 (0.005191)\n",
      "feature v83 (0.005169)\n",
      "feature v59 (0.005128)\n",
      "feature v130 (0.005108)\n",
      "feature v94 (0.005043)\n",
      "feature v104 (0.004961)\n",
      "feature v33 (0.004953)\n",
      "feature v13 (0.004947)\n",
      "feature v68 (0.004946)\n",
      "feature v19 (0.004895)\n",
      "feature v11 (0.004892)\n",
      "feature v77 (0.004703)\n",
      "feature v42 (0.004696)\n",
      "feature v61 (0.004640)\n",
      "feature v93 (0.004614)\n",
      "feature v64 (0.004583)\n",
      "feature v17 (0.004578)\n",
      "feature v76 (0.004564)\n",
      "feature v49 (0.004545)\n",
      "feature v106 (0.004515)\n",
      "feature v20 (0.004503)\n",
      "feature v67 (0.004425)\n",
      "feature v41 (0.004405)\n",
      "feature v96 (0.004379)\n",
      "feature v48 (0.004345)\n",
      "feature v65 (0.004274)\n",
      "feature v29 (0.004265)\n",
      "feature v38 (0.003466)\n",
      "feature v3 (0.002212)\n",
      "feature v74 (0.001000)\n"
     ]
    }
   ],
   "source": [
    "importances = extc.feature_importances_\n",
    "imp_features = []\n",
    "indices = np.argsort(importances)[::-1]\n",
    "for i in range(nonan_train.shape[1]):\n",
    "    imp_features.append(nonan_train.columns[indices[i]])\n",
    "    print(\"feature %s (%f)\" % (nonan_train.columns[indices[i]], importances[indices[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(clf_ext_preds).to_csv('CV_EXTREES.csv', index=False)\n",
    "pd.DataFrame(clf_xgb_preds).to_csv('CV_XGB_CLF.csv', index=False)\n",
    "pd.DataFrame(areg_xgb_preds).to_csv('CV_XGB_REG.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_ext_preds = pd.read_csv('CV_EXTREES.csv')['0'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45618779389320396"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, (clf_ext_preds - clf_ext_preds.min()) / (clf_ext_preds.max() - clf_ext_preds.min()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46334877936\n",
      "28.1292908669\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "extc = RandomForestClassifier(n_estimators=1200,\n",
    "                            max_features=30,\n",
    "                            criterion='entropy',\n",
    "                            min_samples_split=2,\n",
    "                            max_depth=30, \n",
    "                            min_samples_leaf=2, \n",
    "                            n_jobs=-1)    \n",
    "clf_ext_preds, y_test = nocv_loss(train, Y_train, extc, 1)\n",
    "print (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metafeatures(X_train, X_test, targets, clfs):\n",
    "    n_folds = 5\n",
    "    verbose = True\n",
    "    shuffle = False\n",
    "\n",
    "    X, y, X_submission = X_train, targets, X_test\n",
    "\n",
    "    if shuffle:\n",
    "        idx = np.random.permutation(y.size)\n",
    "        X = X[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "    skf = list(StratifiedKFold(y, n_folds))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))\n",
    "    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))\n",
    "\n",
    "    for j, clf in enumerate(clfs):\n",
    "        print j, clf\n",
    "        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))\n",
    "        for i, (train, test) in enumerate(skf):\n",
    "            print i\n",
    "            Xtrain = X[train]\n",
    "            y_train = y[train]\n",
    "            Xtest = X[test]\n",
    "            y_test = y[test]\n",
    "        \n",
    "            clf.fit(Xtrain, y_train)\n",
    "            y_submission = clf.predict_proba(Xtest)[:,1]\n",
    "            dataset_blend_train[test, j] = y_submission\n",
    "            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,1]\n",
    "        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)\n",
    "    return dataset_blend_train, dataset_blend_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = sps.hstack((nonan_train, count_train, enc_train, fac_pair_train, nan_train), format='csr')\n",
    "X_test = sps.hstack((nonan_test, count_test, enc_test, fac_pair_test, nan_test), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-53ddf072b075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnonan_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcat_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuper_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfac_pair_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnonan_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menc_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfac_pair_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnan_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amiras/anaconda2/lib/python2.7/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \"\"\"\n\u001b[1;32m--> 464\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amiras/anaconda2/lib/python2.7/site-packages/scipy/sparse/construct.pyc\u001b[0m in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    621\u001b[0m                 \u001b[0mnnz\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amiras/anaconda2/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36masformat\u001b[1;34m(self, format)\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'to'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;31m###################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/amiras/anaconda2/lib/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36mtocsr\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    336\u001b[0m                       \u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                       \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m                       data)\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = sps.hstack((nonan_train[cat_features], count_train, super_train, fac_pair_train), format='csr')\n",
    "X_test = sps.hstack((nonan_test, count_test, enc_test, fac_pair_test, nan_test), format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "0.453094681269\n",
      "Time: 33.8757515987\n"
     ]
    }
   ],
   "source": [
    "xgb_reg = xgb.XGBRegressor(colsample_bytree=0.4,\n",
    "                                colsample_bylevel=0.7,\n",
    "                                learning_rate=0.0095,\n",
    "                                max_depth=15,\n",
    "                                n_estimators=750,\n",
    "                                nthread=4,\n",
    "                                objective='reg:linear',\n",
    "                                silent=1,\n",
    "                                subsample=0.8,\n",
    "                                min_child_weight=6)\n",
    "\n",
    "start = time.time()\n",
    "areg_xgb_preds, y_test = nocv_loss(X_train, Y_train, xgb_reg, 1, reg=True)\n",
    "areg_xgb_preds[areg_xgb_preds > 0.99] = 0.98\n",
    "print log_loss(y_test, areg_xgb_preds)\n",
    "print 'Time:', (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.453137222615\n"
     ]
    }
   ],
   "source": [
    "areg_xgb_preds[areg_xgb_preds >= 0.97] = 0.98\n",
    "print log_loss(y_test, areg_xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45239517589\n",
      "33.8972441514\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(colsample_bytree=0.9,\n",
    "                            colsample_bylevel=0.2,\n",
    "                            learning_rate=0.0095,\n",
    "                            max_depth=15,\n",
    "                            n_estimators=1000,\n",
    "                            nthread=4,\n",
    "                            objective='binary:logistic',\n",
    "                            silent=1,\n",
    "                            subsample=0.85,\n",
    "                            min_child_weight=3)\n",
    "\n",
    "start = time.time()\n",
    "clf_xgb_preds, y_test = nocv_loss(X_train, Y_train, xgb_clf, 1)\n",
    "print (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0095*750 / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "from sklearn import ensemble, metrics, linear_model\n",
    "import random\n",
    "\n",
    "#Some parameters to play with\n",
    "rnd=12\n",
    "random.seed(rnd)\n",
    "n_ft=20 #Number of features to add\n",
    "max_elts=3 #Maximum size of a group of linear features\n",
    "\n",
    "class addNearestNeighbourLinearFeatures:\n",
    "    \n",
    "    def __init__(self, n_neighbours=1, max_elts=None, verbose=True, random_state=None):\n",
    "        self.rnd=random_state\n",
    "        self.n=n_neighbours\n",
    "        self.max_elts=max_elts\n",
    "        self.verbose=verbose\n",
    "        self.neighbours=[]\n",
    "        self.clfs=[]\n",
    "        \n",
    "    def fit(self,train,y):\n",
    "        if self.rnd!=None:\n",
    "            random.seed(rnd)\n",
    "        if self.max_elts==None:\n",
    "            self.max_elts=len(train.columns)\n",
    "        list_vars=list(train.columns)\n",
    "        random.shuffle(list_vars)\n",
    "        \n",
    "        lastscores=np.zeros(self.n)+1e15\n",
    "\n",
    "        for elt in list_vars[:self.n]:\n",
    "            self.neighbours.append([elt])\n",
    "        list_vars=list_vars[self.n:]\n",
    "        \n",
    "        for elt in list_vars:\n",
    "            indice=0\n",
    "            scores=[]\n",
    "            for elt2 in self.neighbours:\n",
    "                if len(elt2)<self.max_elts:\n",
    "                    clf=xgb.XGBRegressor(\n",
    "                                colsample_bylevel=0.7,\n",
    "                                learning_rate=0.07125,\n",
    "                                max_depth=15,\n",
    "                                n_estimators=100,\n",
    "                                nthread=4,\n",
    "                                objective='reg:linear',\n",
    "                                silent=1,\n",
    "                                subsample=0.8,\n",
    "                                min_child_weight=6)\n",
    "                    \n",
    "                    clf.fit(train[elt2+[elt]], y)\n",
    "                    scores.append(metrics.log_loss(y,clf.predict(train[elt2 + [elt]])))\n",
    "                    indice=indice+1\n",
    "                else:\n",
    "                    scores.append(lastscores[indice])\n",
    "                    indice=indice+1\n",
    "            gains=lastscores-scores\n",
    "            if gains.max()>0:\n",
    "                temp=gains.argmax()\n",
    "                lastscores[temp]=scores[temp]\n",
    "                self.neighbours[temp].append(elt)\n",
    "\n",
    "        indice=0\n",
    "        for elt in self.neighbours:\n",
    "            clf=xgb.XGBRegressor(\n",
    "                                colsample_bylevel=0.7,\n",
    "                                learning_rate=0.07125,\n",
    "                                max_depth=15,\n",
    "                                n_estimators=100,\n",
    "                                nthread=4,\n",
    "                                objective='reg:linear',\n",
    "                                silent=1,\n",
    "                                subsample=0.8,\n",
    "                                min_child_weight=6)\n",
    "            clf.fit(train[elt], y)\n",
    "            self.clfs.append(clf)\n",
    "            if self.verbose:\n",
    "                print(indice, lastscores[indice], elt)\n",
    "            indice=indice+1\n",
    "                    \n",
    "    def transform(self, train):\n",
    "        indice=0\n",
    "        for elt in self.neighbours:\n",
    "            train['_'.join(pd.Series(elt).sort_values().values)]=self.clfs[indice].predict(train[elt])\n",
    "            indice=indice+1\n",
    "        return train\n",
    "    \n",
    "    def fit_transform(self, train, y):\n",
    "        self.fit(train, y)\n",
    "        return self.transform(train)\n",
    "    \n",
    "    \n",
    "train = df_train\n",
    "target = Y_train\n",
    "test = df_test\n",
    "id_test = id_test\n",
    "\n",
    "train['v22-1']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "test['v22-1']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[0]))\n",
    "train['v22-2']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "test['v22-2']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[1]))\n",
    "train['v22-3']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "test['v22-3']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[2]))\n",
    "train['v22-4']=train['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "test['v22-4']=test['v22'].fillna('@@@@').apply(lambda x:'@'*(4-len(str(x)))+str(x)).apply(lambda x:ord(x[3]))\n",
    "\n",
    "drop_list=['v91','v1', 'v8', 'v10', 'v15', 'v17', 'v25', 'v29', 'v34', 'v41', 'v46', 'v54', 'v64', 'v67', 'v97', 'v105', 'v111', 'v122']\n",
    "train = train.drop(drop_list,axis=1).fillna(-999)\n",
    "test = test.drop(drop_list,axis=1).fillna(-999)\n",
    "\n",
    "refcols=list(train.columns)\n",
    "\n",
    "for elt in refcols:\n",
    "    if train[elt].dtype=='O':\n",
    "        train[elt], temp = pd.factorize(train[elt])\n",
    "        test[elt]=temp.get_indexer(test[elt])\n",
    "    else:\n",
    "        train[elt]=train[elt].round(5)\n",
    "        test[elt]=test[elt].round(5)\n",
    "        \n",
    "a=addNearestNeighbourLinearFeatures(n_neighbours=n_ft, max_elts=max_elts, verbose=True, random_state=rnd)\n",
    "a.fit(train, target)\n",
    "\n",
    "train = a.transform(train)\n",
    "test = a.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_new = list(set(train.columns) - set(df_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_train = train[linear_new]\n",
    "linear_test = test[linear_new]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.454062622178\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.ExtraTreesClassifier(n_estimators=1200,max_features=50,criterion= 'entropy',min_samples_split= 4,\n",
    "                        max_depth= 35, min_samples_leaf= 2, n_jobs = -1, random_state=rnd)\n",
    "\n",
    "nclf_ext_preds, y_test = nocv_loss(train, Y_train, clf, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.489410291235\n",
      "25.2838318984\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(colsample_bytree=0.9,\n",
    "                            colsample_bylevel=0.9,\n",
    "                            learning_rate=0.0095,\n",
    "                            max_depth=9,\n",
    "                            n_estimators=1000,\n",
    "                            nthread=4,\n",
    "                            objective='binary:logistic',\n",
    "                            silent=1,\n",
    "                            subsample=0.9,\n",
    "                            min_child_weight=1)\n",
    "\n",
    "start = time.time()\n",
    "sclf_xgb_preds, y_test = nocv_loss(X_train, Y_train, xgb_clf, 1)\n",
    "print (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.480531062899\n",
      "1.46316971779\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier(colsample_bytree=0.2,\n",
    "                            colsample_bylevel=0.2,\n",
    "                            learning_rate=0.0095,\n",
    "                            max_depth=15,\n",
    "                            n_estimators=300,\n",
    "                            nthread=4,\n",
    "                            objective='binary:logistic',\n",
    "                            silent=1,\n",
    "                            subsample=0.85,\n",
    "                            min_child_weight=3)\n",
    "\n",
    "start = time.time()\n",
    "sclf_xgb_preds, y_test = nocv_loss(X_train[:45000], Y_train[:45000], xgb_clf, 1)\n",
    "print (time.time() - start) / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "0.47384843154\n",
    "3.98084379832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def blending(targets, preds1, preds2, p):\n",
    "    print 'p:', p, log_loss(targets, preds1*p + preds2*(1-p))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0 1.0 0.453094681393\n",
      "0.0 0.1 0.9 0.451899259129\n",
      "0.0 0.2 0.8 0.450908491289\n",
      "0.0 0.3 0.7 0.450132092501\n",
      "0.0 0.4 0.6 0.44958225219\n",
      "0.0 0.5 0.5 0.449274766165\n",
      "0.0 0.6 0.4 0.449229953446\n",
      "0.0 0.7 0.3 0.449474455858\n",
      "0.0 0.8 0.2 0.45004437333\n",
      "0.0 0.9 0.1 0.450991122205\n",
      "0.0 1.0 0.0 0.452395176051\n",
      "0.1 0.0 0.9 0.45204919472\n",
      "0.1 0.1 0.8 0.450899520043\n",
      "0.1 0.2 0.7 0.4499533474\n",
      "0.1 0.3 0.6 0.449221216244\n",
      "0.1 0.4 0.5 0.448716308531\n",
      "0.1 0.5 0.4 0.448455573737\n",
      "0.1 0.6 0.3 0.448460918479\n",
      "0.1 0.7 0.2 0.448761480206\n",
      "0.1 0.8 0.1 0.449397207994\n",
      "0.1 0.9 0.0 0.450427594013\n",
      "0.2 0.0 0.8 0.4513005497\n",
      "0.2 0.1 0.7 0.450197789733\n",
      "0.2 0.2 0.6 0.449299395046\n",
      "0.2 0.3 0.5 0.448616740214\n",
      "0.2 0.4 0.4 0.448164285206\n",
      "0.2 0.5 0.3 0.447960663452\n",
      "0.2 0.6 0.2 0.448030104161\n",
      "0.2 0.7 0.1 0.448405432916\n",
      "0.2 0.8 0.0 0.449133964348\n",
      "0.3 0.0 0.7 0.450815484538\n",
      "0.3 0.1 0.6 0.449762777223\n",
      "0.3 0.2 0.5 0.448916799093\n",
      "0.3 0.3 0.4 0.44829013256\n",
      "0.3 0.4 0.3 0.4478988412\n",
      "0.3 0.5 0.2 0.447763653801\n",
      "0.3 0.6 0.1 0.447912456865\n",
      "0.3 0.7 -1.11022302463e-16 0.448384570013\n",
      "0.4 0.0 0.6 0.450575498062\n",
      "0.4 0.1 0.5 0.449577157095\n",
      "0.4 0.2 0.4 0.448789498819\n",
      "0.4 0.3 0.3 0.448226442388\n",
      "0.4 0.4 0.2 0.447906186961\n",
      "0.4 0.5 0.1 0.447852941298\n",
      "0.4 0.6 -1.11022302463e-16 0.448100454877\n",
      "0.5 0.0 0.5 0.450570320327\n",
      "0.5 0.1 0.4 0.449631747542\n",
      "0.5 0.2 0.3 0.448909208631\n",
      "0.5 0.3 0.2 0.448418834159\n",
      "0.5 0.4 0.1 0.448182034649\n",
      "0.5 0.5 0.0 0.448228494957\n",
      "0.6 0.0 0.4 0.450795000464\n",
      "0.6 0.1 0.3 0.449922559713\n",
      "0.6 0.2 0.2 0.4492737305\n",
      "0.6 0.3 0.1 0.44886772711\n",
      "0.6 0.4 -1.11022302463e-16 0.448731211736\n",
      "0.7 0.0 0.3 0.451248961577\n",
      "0.7 0.1 0.2 0.450450851523\n",
      "0.7 0.2 0.1 0.449886961274\n",
      "0.7 0.3 -1.11022302463e-16 0.449581681255\n",
      "0.8 0.0 0.2 0.451936346866\n",
      "0.8 0.1 0.1 0.451223539419\n",
      "0.8 0.2 -5.55111512313e-17 0.450760853109\n",
      "0.9 0.0 0.1 0.452867239084\n",
      "0.9 0.1 -2.77555756156e-17 0.452256301432\n",
      "1.0 0.0 0.0 0.454062622178\n"
     ]
    }
   ],
   "source": [
    "for p in np.arange(0, 1.05, 0.1):\n",
    "    for q in np.arange(0, 1.05 - p, 0.1):\n",
    "        print p, q, 1-p-q, log_loss(y_test, p*nclf_ext_preds + q*clf_xgb_preds + (1 - p - q)*areg_xgb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.0 0.453094681269\n",
      "p: 0.05 0.452471879375\n",
      "p: 0.1 0.451899259238\n",
      "p: 0.15 0.451377758719\n",
      "p: 0.2 0.450908491194\n",
      "p: 0.25 0.450492799703\n",
      "p: 0.3 0.450132092031\n",
      "p: 0.35 0.449827966103\n",
      "p: 0.4 0.449582251864\n",
      "p: 0.45 0.449397060069\n",
      "p: 0.5 0.449274765864\n",
      "p: 0.55 0.449218037244\n",
      "p: 0.6 0.449229953233\n",
      "p: 0.65 0.449314052684\n",
      "p: 0.7 0.449474455927\n",
      "p: 0.75 0.449715977026\n",
      "p: 0.8 0.450044373733\n",
      "p: 0.85 0.450466580091\n",
      "p: 0.9 0.450991121728\n",
      "p: 0.95 0.451629018197\n",
      "p: 1.0 0.45239517589\n"
     ]
    }
   ],
   "source": [
    "for p in np.arange(0, 1.05, 0.05):\n",
    "    blending(y_test, clf_xgb_preds, areg_xgb_preds, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.round(enc_xgb_preds, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56132094504557029"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBRegressor(colsample_bytree=0.4,\n",
    "                                colsample_bylevel=0.7,\n",
    "                                learning_rate=0.0095,\n",
    "                                max_depth=15,\n",
    "                                n_estimators=750,\n",
    "                                nthread=4,\n",
    "                                objective='reg:linear',\n",
    "                                silent=1,\n",
    "                                subsample=0.8,\n",
    "                                min_child_weight=6)\n",
    "\n",
    "rxgbypred = np.zeros(X_test.shape[0])\n",
    "for i in range(1):\n",
    "    print i\n",
    "    clf.fit(X_train, Y_train)\n",
    "    rxgbypred += clf.predict(X_test)\n",
    "rxgbypred /= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rxgbypred += clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(colsample_bytree=0.9,\n",
    "                            colsample_bylevel=0.2,\n",
    "                            learning_rate=0.0095,\n",
    "                            max_depth=15,\n",
    "                            n_estimators=1000,\n",
    "                            nthread=4,\n",
    "                            objective='binary:logistic',\n",
    "                            silent=1,\n",
    "                            subsample=0.85,\n",
    "                            min_child_weight=3)\n",
    "\n",
    "xgbypred = np.zeros(X_test.shape[0])\n",
    "for i in range(1):\n",
    "    print i\n",
    "    clf.fit(X_train, Y_train)\n",
    "    xgbypred += clf.predict_proba(X_test)[:, 1]\n",
    "xgbypred /= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rxgbypred[rxgbypred > 0.99] = 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=1200,\n",
    "                            max_features=30,\n",
    "                            criterion='entropy',\n",
    "                            min_samples_split=2,\n",
    "                            max_depth=30, \n",
    "                            min_samples_leaf=2, \n",
    "                            n_jobs=-1) \n",
    "exypred = np.zeros(test.shape[0])\n",
    "for i in range(1):\n",
    "    print i\n",
    "    clf.fit(train, Y_train)\n",
    "    exypred += clf.predict_proba(test)[:, 1]\n",
    "exypred /= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.ExtraTreesClassifier(n_estimators=1200,max_features=50,criterion= 'entropy',min_samples_split= 4,\n",
    "                        max_depth= 35, min_samples_leaf= 2, n_jobs = -1, random_state=rnd)\n",
    "\n",
    "exypred = np.zeros(test.shape[0])\n",
    "for i in range(1):\n",
    "    print i\n",
    "    clf.fit(train, Y_train)\n",
    "    exypred += clf.predict_proba(test)[:, 1]\n",
    "exypred /= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_submission(pred, id_test, filename):\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ID'] = id_test\n",
    "    df_pred['PredictedProb'] = pred\n",
    "\n",
    "    df_pred.to_csv(filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_submission(exypred*0.5 + 0.5*xgbypred, id_test, 'best.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_submission(xgbypred, id_test, 'xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rxgbypred[rxgbypred >= 0.99] = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best = pd.read_csv('EX3XGB5REG2.csv')['PredictedProb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blend_xgb_extra = pextra['PredictedProb']*0.4 + pxgb['PredictedProb']*0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
